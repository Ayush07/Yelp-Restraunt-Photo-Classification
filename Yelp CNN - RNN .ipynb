{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 1, 0], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell to verify data formats\n",
    "test3 = pd.read_csv('/Users/JosephVele/Downloads/all/train_photo_to_biz_ids.csv')\n",
    "test4 = test3[test3['photo_id']==5].iloc[:,3:12].values[0].tolist()\n",
    "test3.iloc[0,3:12].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing cell not used - Used for testing when creating data loader\n",
    "path = '/Users/JosephVele/Downloads/all/train_photo_to_biz_ids.csv'\n",
    "train_data = pd.read_csv(path,usecols=range(1),dtype = 'str') #nameframe\n",
    "label_data=pd.read_csv(path,sep=\",\", usecols=[2], nrows=1000) #label_frame\n",
    "root = '/Users/JosephVele/Downloads/all/train_photos'\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "pic_name = os.path.join(root, train_data.iloc[1, 0]+'.jpg')\n",
    "pic = Image.open(pic_name)\n",
    "transform(pic)\n",
    "\n",
    "np.array(label_data.iloc[1,0].split()).astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yelpDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self,text_file,root_dir, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file(string): path to text file\n",
    "            root_dir(string): directory with all train images\n",
    "        \"\"\"\n",
    "        \n",
    "        self.name_frame = pd.read_csv(text_file,sep=\",\",usecols=range(1),dtype = 'str',nrows = 10000)\n",
    "        self.label_frame = pd.read_csv(text_file,sep=\",\",  nrows=10000)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "                                       \n",
    "    def __len__(self):\n",
    "        return len(self.name_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #photoid = self.name_frame.iloc[idx, 0]\n",
    "        img_name = os.path.join(self.root_dir, self.name_frame.iloc[idx, 0]  +'.jpg')\n",
    "        #print(img_name)\n",
    "        image = Image.open(img_name)\n",
    "        image = image.convert('RGB')\n",
    "        image = self.transform(image) \n",
    "        labels = self.label_frame.iloc[idx,3:12].values\n",
    "        labels = np.array(labels)\n",
    "        labels= torch.from_numpy(labels.astype('int'))\n",
    "        #print(labels)\n",
    "        #labels = self.label_frame.iloc[idx,0]\n",
    "        #labels = labels.reshape(-1, 2)\n",
    "        sample = {'image': image, 'labels': labels}\n",
    "        \n",
    "        return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelpTrainSet = yelpDataset(text_file ='/Users/JosephVele/Downloads/all/train_photo_to_biz_ids.csv',\n",
    "                           root_dir = '/Users/JosephVele/Downloads/all/train_photos',\n",
    "                          transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                                          transforms.ToTensor(),\n",
    "                                                          transforms.Normalize(\n",
    "                                                              mean = [0.485, 0.456, 0.406],\n",
    "                                                              std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "yelpTrainLoader = torch.utils.data.DataLoader(yelpTrainSet,batch_size=10,shuffle=True, num_workers=0)\n",
    "\n",
    "yelpTestSet = yelpDataset(text_file ='/Users/JosephVele/Downloads/all/test_photo_to_biz_ids.csv',\n",
    "                          root_dir = '/Users/JosephVele/Downloads/all/train_photos',\n",
    "                          transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                                          transforms.ToTensor(),\n",
    "                                                          transforms.Normalize(\n",
    "                                                              mean = [0.485, 0.456, 0.406],\n",
    "                                                              std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "yelpTestLoader = torch.utils.data.DataLoader(yelpTrainSet,batch_size=10,shuffle=True, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Batch Size:10 \t Image Size: torch.Size([3, 224, 224]) \t Number of Labels: 9\n",
      "Batch: 1 \t Batch Size:10 \t Image Size: torch.Size([3, 224, 224]) \t Number of Labels: 9\n",
      "Batch: 2 \t Batch Size:10 \t Image Size: torch.Size([3, 224, 224]) \t Number of Labels: 9\n",
      "Batch: 3 \t Batch Size:10 \t Image Size: torch.Size([3, 224, 224]) \t Number of Labels: 9\n",
      "Batch: 4 \t Batch Size:10 \t Image Size: torch.Size([3, 224, 224]) \t Number of Labels: 9\n",
      "Batch: 5 \t Batch Size:10 \t Image Size: torch.Size([3, 224, 224]) \t Number of Labels: 9\n",
      "Batch: 6 \t Batch Size:10 \t Image Size: torch.Size([3, 224, 224]) \t Number of Labels: 9\n",
      "Batch: 7 \t Batch Size:10 \t Image Size: torch.Size([3, 224, 224]) \t Number of Labels: 9\n",
      "Batch: 8 \t Batch Size:10 \t Image Size: torch.Size([3, 224, 224]) \t Number of Labels: 9\n",
      "Batch: 9 \t Batch Size:10 \t Image Size: torch.Size([3, 224, 224]) \t Number of Labels: 9\n"
     ]
    }
   ],
   "source": [
    "for i_batch,sample_batched in enumerate(yelpTrainLoader,0):\n",
    "    batch_size =sample_batched['image'].shape[0]\n",
    "    image_size = sample_batched['image'].shape[1:4]\n",
    "    n_labels = sample_batched['labels'].shape[1]\n",
    "    print('Batch: {} \\t Batch Size:{} \\t Image Size: {} \\t Number of Labels: {}'.\n",
    "          format(i_batch, batch_size, image_size,n_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No longer needed since we are normalizing based off the pretrained models\n",
    "#Calculate mean &std\n",
    "#x=[]\n",
    "#for i_batch,sample_batched in enumerate(yelpTrainLoader,0):\n",
    "#    numpy_image = sample_batched['image'].numpy()\n",
    "#    x.append(np.mean(numpy_image, axis=(0,2,3)))\n",
    "\n",
    "#image_mean=np.mean(x, axis= (0))\n",
    "#image_std=np.std(x, axis=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No longer needed since we are normalizing based off the pretrained models\n",
    "#Confirm images have been normalized appropriately with 0 mean and 1 std\n",
    "#print(image_mean)\n",
    "#print(image_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG19 with Batch Normilzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AlexNet and VGGNet with batch Normilization\n",
    "import torchvision.models as models\n",
    "vgg19 = models.vgg19_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify VGGNET19\n",
    "#Don't update the existing parameters \n",
    "\n",
    "for param in vgg19.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#By default this requires_grad is set to true\n",
    "vgg19.classifier._modules['6']  = nn.Linear(4096, 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyper-parameters\n",
    "#******************************#\n",
    "learning = .01 # Learning Rate - chosen using grid search at end\n",
    "moment =.9 #Momentum - chosen using grid search at end\n",
    "\n",
    "#******************************#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Define criterion and optimizer\n",
    "#************************************#\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(vgg19.classifier._modules['6'].parameters(), lr=learning, momentum=moment)\n",
    "\n",
    "#************************************#\n",
    "\n",
    "# 4.2 Train the model\n",
    "# 4.3 Please store and print training and validation loss&accuracy after each epoch\n",
    "#********************************************#\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "acc_data_train=[]\n",
    "acc_data_test=[]\n",
    "\n",
    "def train(epoch):\n",
    "    vgg19.train()\n",
    "    train_loss = 0\n",
    "    TN = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i, sample_batched in enumerate(yelpTrainLoader,1):\n",
    "        inputs = sample_batched['image']\n",
    "        labels = sample_batched['labels']\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = vgg19(inputs)\n",
    "        loss = criterion(outputs.float(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss+= loss.item()\n",
    "        pred = (torch.sigmoid(outputs).data > 0.5).int()\n",
    "        #print(pred)\n",
    "        labels = labels.int()\n",
    "        #print(labels)\n",
    "        \n",
    "        TP += ((pred == 1) & (labels == 1) ).float().sum() #True Positive Count\n",
    "        TN += ((pred == 0) & (labels == 0) ).float().sum() #True Negative Count\n",
    "        FP += ((pred == 1) & (labels == 0) ).float().sum() #False Positive Count\n",
    "        FN += ((pred == 0) & (labels == 1) ).float().sum() #False Negative Count\n",
    "        #print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP,TN,FP,FN) )    \n",
    "    \n",
    "    accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "    precision = TP/(TP + FP)\n",
    "    recall = TP/(TP + FN)\n",
    "    f1_score = 2 * (precision*recall)/(precision+recall)\n",
    "    acc_data_train.append([TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "    train_loss=float(train_loss)/float(i)\n",
    "    train_losses.append(train_loss)\n",
    "    # print statistics        \n",
    "    print('Train Epoch:{}  Accuracy: {:.2f}   Average Loss: {:.2f}  Precision: {:.2f}   F1 Score: {:.2f}\\n'.\n",
    "          format(epoch, accuracy, train_loss, precision, f1_score))\n",
    "    \n",
    "def test(epoch):\n",
    "    #Have our model in evaluation mode\n",
    "    vgg19.eval()\n",
    "    #Set losses and Correct labels to zero\n",
    "    test_loss = 0\n",
    "    TN = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(yelpTestLoader,1):\n",
    "            inputs = sample_batched['image']\n",
    "            labels = sample_batched['labels']\n",
    "            outputs = vgg19(inputs)\n",
    "            loss = criterion(outputs.float(), labels.float())\n",
    "            test_loss += loss.item()\n",
    "            pred = (torch.sigmoid(outputs).data > 0.5).int()\n",
    "            #print(pred)\n",
    "            labels = labels.int()\n",
    "            #print(labels)\n",
    "\n",
    "            TP += ((pred == 1) & (labels == 1) ).float().sum() #True Positive Count\n",
    "            TN += ((pred == 0) & (labels == 0) ).float().sum() #True Negative Count\n",
    "            FP += ((pred == 1) & (labels == 0) ).float().sum() #False Positive Count\n",
    "            FN += ((pred == 0) & (labels == 1) ).float().sum() #False Negative Count\n",
    "            #print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP,TN,FP,FN) )    \n",
    "\n",
    "        accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "        precision = TP/(TP + FP)\n",
    "        recall = TP/(TP + FN)\n",
    "        f1_score = 2 * (precision*recall)/(precision+recall)\n",
    "        acc_data_test.append([TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "        test_loss = float(test_loss)/float(i)\n",
    "        test_losses.append(test_loss)\n",
    "        # print statistics        \n",
    "        print('Test Epoch:{}  Accuracy: {:.2f}   Average Loss: {:.2f}  Precision: {:.2f}   F1 Score: {:.2f}\\n'.\n",
    "              format(epoch, accuracy, test_loss, precision, f1_score))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: SGD\tLearning Rate: 0.1\tMomentum: 0.3\n",
      "Train Epoch:1  Accuracy: 0.61   Average Loss: 0.66  Precision: 0.62   F1 Score: 0.63\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.1\tMomentum: 0.4\n",
      "Train Epoch:1  Accuracy: 0.60   Average Loss: 0.67  Precision: 0.61   F1 Score: 0.62\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.1\tMomentum: 0.5\n",
      "Train Epoch:1  Accuracy: 0.60   Average Loss: 0.70  Precision: 0.61   F1 Score: 0.60\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.1\tMomentum: 0.6\n",
      "Train Epoch:1  Accuracy: 0.55   Average Loss: 0.71  Precision: 0.56   F1 Score: 0.58\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.1\tMomentum: 0.7\n",
      "Train Epoch:1  Accuracy: 0.64   Average Loss: 0.66  Precision: 0.64   F1 Score: 0.65\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.1\tMomentum: 0.8\n",
      "Train Epoch:1  Accuracy: 0.60   Average Loss: 0.75  Precision: 0.60   F1 Score: 0.62\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.1\tMomentum: 0.9\n",
      "Train Epoch:1  Accuracy: 0.60   Average Loss: 0.72  Precision: 0.61   F1 Score: 0.61\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.01\tMomentum: 0.3\n",
      "Train Epoch:1  Accuracy: 0.56   Average Loss: 0.69  Precision: 0.57   F1 Score: 0.59\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.01\tMomentum: 0.4\n",
      "Train Epoch:1  Accuracy: 0.56   Average Loss: 0.68  Precision: 0.56   F1 Score: 0.60\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.01\tMomentum: 0.5\n",
      "Train Epoch:1  Accuracy: 0.54   Average Loss: 0.69  Precision: 0.55   F1 Score: 0.57\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.01\tMomentum: 0.6\n",
      "Train Epoch:1  Accuracy: 0.58   Average Loss: 0.68  Precision: 0.60   F1 Score: 0.60\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.01\tMomentum: 0.7\n",
      "Train Epoch:1  Accuracy: 0.58   Average Loss: 0.67  Precision: 0.61   F1 Score: 0.57\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.01\tMomentum: 0.8\n",
      "Train Epoch:1  Accuracy: 0.56   Average Loss: 0.68  Precision: 0.57   F1 Score: 0.58\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.01\tMomentum: 0.9\n",
      "Train Epoch:1  Accuracy: 0.57   Average Loss: 0.69  Precision: 0.57   F1 Score: 0.60\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.001\tMomentum: 0.3\n",
      "Train Epoch:1  Accuracy: 0.53   Average Loss: 0.69  Precision: 0.54   F1 Score: 0.58\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.001\tMomentum: 0.4\n",
      "Train Epoch:1  Accuracy: 0.50   Average Loss: 0.71  Precision: 0.52   F1 Score: 0.50\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.001\tMomentum: 0.5\n",
      "Train Epoch:1  Accuracy: 0.55   Average Loss: 0.69  Precision: 0.57   F1 Score: 0.54\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.001\tMomentum: 0.6\n",
      "Train Epoch:1  Accuracy: 0.52   Average Loss: 0.69  Precision: 0.53   F1 Score: 0.53\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.001\tMomentum: 0.7\n",
      "Train Epoch:1  Accuracy: 0.50   Average Loss: 0.71  Precision: 0.52   F1 Score: 0.56\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.001\tMomentum: 0.8\n",
      "Train Epoch:1  Accuracy: 0.49   Average Loss: 0.71  Precision: 0.51   F1 Score: 0.51\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.001\tMomentum: 0.9\n",
      "Train Epoch:1  Accuracy: 0.55   Average Loss: 0.70  Precision: 0.56   F1 Score: 0.57\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 1e-05\tMomentum: 0.3\n",
      "Train Epoch:1  Accuracy: 0.52   Average Loss: 0.71  Precision: 0.53   F1 Score: 0.52\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 1e-05\tMomentum: 0.4\n",
      "Train Epoch:1  Accuracy: 0.48   Average Loss: 0.71  Precision: 0.50   F1 Score: 0.43\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 1e-05\tMomentum: 0.5\n",
      "Train Epoch:1  Accuracy: 0.50   Average Loss: 0.71  Precision: 0.52   F1 Score: 0.48\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 1e-05\tMomentum: 0.6\n",
      "Train Epoch:1  Accuracy: 0.49   Average Loss: 0.71  Precision: 0.51   F1 Score: 0.54\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 1e-05\tMomentum: 0.7\n",
      "Train Epoch:1  Accuracy: 0.50   Average Loss: 0.70  Precision: 0.52   F1 Score: 0.47\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 1e-05\tMomentum: 0.8\n",
      "Train Epoch:1  Accuracy: 0.47   Average Loss: 0.73  Precision: 0.48   F1 Score: 0.47\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 1e-05\tMomentum: 0.9\n",
      "Train Epoch:1  Accuracy: 0.48   Average Loss: 0.72  Precision: 0.49   F1 Score: 0.52\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.1\t WeightDecay: 0.1\n",
      "Train Epoch:1  Accuracy: 0.56   Average Loss: 3.77  Precision: 0.58   F1 Score: 0.57\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.1\t WeightDecay: 0.01\n",
      "Train Epoch:1  Accuracy: 0.58   Average Loss: 7.65  Precision: 0.59   F1 Score: 0.60\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.1\t WeightDecay: 0.001\n",
      "Train Epoch:1  Accuracy: 0.57   Average Loss: 6.50  Precision: 0.58   F1 Score: 0.59\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.01\t WeightDecay: 0.1\n",
      "Train Epoch:1  Accuracy: 0.58   Average Loss: 1.06  Precision: 0.59   F1 Score: 0.59\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.01\t WeightDecay: 0.01\n",
      "Train Epoch:1  Accuracy: 0.52   Average Loss: 1.49  Precision: 0.54   F1 Score: 0.52\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.01\t WeightDecay: 0.001\n",
      "Train Epoch:1  Accuracy: 0.56   Average Loss: 1.16  Precision: 0.57   F1 Score: 0.57\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.001\t WeightDecay: 0.1\n",
      "Train Epoch:1  Accuracy: 0.57   Average Loss: 0.70  Precision: 0.57   F1 Score: 0.60\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.001\t WeightDecay: 0.01\n",
      "Train Epoch:1  Accuracy: 0.60   Average Loss: 0.69  Precision: 0.61   F1 Score: 0.62\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.001\t WeightDecay: 0.001\n",
      "Train Epoch:1  Accuracy: 0.60   Average Loss: 0.68  Precision: 0.61   F1 Score: 0.61\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 1e-05\t WeightDecay: 0.1\n",
      "Train Epoch:1  Accuracy: 0.51   Average Loss: 0.71  Precision: 0.53   F1 Score: 0.47\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 1e-05\t WeightDecay: 0.01\n",
      "Train Epoch:1  Accuracy: 0.51   Average Loss: 0.70  Precision: 0.52   F1 Score: 0.54\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 1e-05\t WeightDecay: 0.001\n",
      "Train Epoch:1  Accuracy: 0.51   Average Loss: 0.71  Precision: 0.53   F1 Score: 0.56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter tuning - Grid Search \n",
    "\n",
    "learning_rates = [.1,.01,.001, .00001]\n",
    "momentums = [.3,.4,.5,.6, .7, .8, .9]\n",
    "w = [.1,.01,.001]\n",
    "\n",
    "for i in [1,2]: #Test different optimizer\n",
    "    for j in range(len(learning_rates)):    \n",
    "        if i ==1:\n",
    "            for k in range(len(momentums)):\n",
    "                #Reset Model per test\n",
    "                vgg19 = models.vgg19_bn(pretrained=True)\n",
    "                for param in vgg19.parameters():\n",
    "                    param.requires_grad = False\n",
    "                vgg19.classifier._modules['6']  = nn.Linear(4096, 9)\n",
    "                \n",
    "                optimizer = optim.SGD(vgg19.classifier._modules['6'].parameters(), \n",
    "                                  lr=learning_rates[j], momentum=momentums[k])\n",
    "                \n",
    "                print('Optimizer: SGD\\tLearning Rate: {}\\tMomentum: {}'.\n",
    "                      format (learning_rates[j], momementums[k]))\n",
    "                train(epoch)\n",
    "                \n",
    "        else: \n",
    "            for k in range(len(w)):\n",
    "                #Reset Model per test\n",
    "                vgg19 = models.vgg19_bn(pretrained=True)\n",
    "                for param in vgg19.parameters():\n",
    "                    param.requires_grad = False\n",
    "                vgg19.classifier._modules['6']  = nn.Linear(4096, 9)\n",
    "                \n",
    "                optimizer = optim.Adam(vgg19.classifier._modules['6'].parameters(), \n",
    "                                  lr=learning_rates[j], weight_decay = w[k])\n",
    "        \n",
    "                print('Optimizer: Adam\\t Learning Rate: {}\\t WeightDecay: {}'.\n",
    "                      format (learning_rates[j], w[k]))\n",
    "                train(epoch)\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch:0  Accuracy: 0.52   Average Loss: 0.70  Precision: 0.53   F1 Score: 0.56\n",
      "\n",
      "Train Epoch:1  Accuracy: 0.59   Average Loss: 0.69  Precision: 0.59   F1 Score: 0.62\n",
      "\n",
      "Test Epoch:1  Accuracy: 0.79   Average Loss: 0.46  Precision: 0.83   F1 Score: 0.78\n",
      "\n",
      "Train Epoch:2  Accuracy: 0.74   Average Loss: 0.51  Precision: 0.75   F1 Score: 0.75\n",
      "\n",
      "Test Epoch:2  Accuracy: 0.88   Average Loss: 0.33  Precision: 0.89   F1 Score: 0.88\n",
      "\n",
      "Train Epoch:3  Accuracy: 0.78   Average Loss: 0.42  Precision: 0.78   F1 Score: 0.79\n",
      "\n",
      "Test Epoch:3  Accuracy: 0.91   Average Loss: 0.27  Precision: 0.93   F1 Score: 0.91\n",
      "\n",
      "Train Epoch:4  Accuracy: 0.84   Average Loss: 0.36  Precision: 0.84   F1 Score: 0.85\n",
      "\n",
      "Test Epoch:4  Accuracy: 0.94   Average Loss: 0.21  Precision: 0.97   F1 Score: 0.94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Finally run model on both training and  test\n",
    "\n",
    "vgg19 = models.vgg19_bn(pretrained=True)\n",
    "for param in vgg19.parameters():\n",
    "    param.requires_grad = False\n",
    "vgg19.classifier._modules['6']  = nn.Linear(4096, 9)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(vgg19.classifier._modules['6'].parameters(), lr=.1, momentum=.7)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "acc_data_train=[]\n",
    "acc_data_test=[]\n",
    "\n",
    "test(0)\n",
    "for epoch in range(1,5):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alexnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alexnet requires 227 x 227\n",
    "yelpTrainSet = yelpDataset(text_file ='/Users/JosephVele/Downloads/all/train_photo_to_biz_ids.csv',\n",
    "                           root_dir = '/Users/JosephVele/Downloads/all/train_photos',\n",
    "                          transform = transforms.Compose([transforms.Resize((227,227)),\n",
    "                                                          transforms.ToTensor(),\n",
    "                                                          transforms.Normalize(\n",
    "                                                              mean = [0.485, 0.456, 0.406],\n",
    "                                                              std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "yelpTrainLoader = torch.utils.data.DataLoader(yelpTrainSet,batch_size=10,shuffle=True, num_workers=0)\n",
    "\n",
    "yelpTestSet = yelpDataset(text_file ='/Users/JosephVele/Downloads/all/test_photo_to_biz_ids.csv',\n",
    "                          root_dir = '/Users/JosephVele/Downloads/all/train_photos',\n",
    "                          transform = transforms.Compose([transforms.Resize((227,227)),\n",
    "                                                          transforms.ToTensor(),\n",
    "                                                          transforms.Normalize(\n",
    "                                                              mean = [0.485, 0.456, 0.406],\n",
    "                                                              std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "yelpTestLoader = torch.utils.data.DataLoader(yelpTrainSet,batch_size=10,shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_net = models.alexnet(pretrained=True)\n",
    "for param in alex_net.parameters():\n",
    "    param.requires_grad = False\n",
    "alex_net.classifier._modules['6'] = nn.Linear(4096, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Define criterion and optimizer\n",
    "#************************************#\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(alex_net.parameters(), lr=learning, momentum=moment)\n",
    "\n",
    "#************************************#\n",
    "\n",
    "# 4.2 Train the model\n",
    "# 4.3 Please store and print training and validation loss&accuracy after each epoch\n",
    "#********************************************#\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "acc_data_train=[]\n",
    "acc_data_test=[]\n",
    "\n",
    "def alex_train(epoch):\n",
    "    alex_net.train()\n",
    "    train_loss = 0\n",
    "    TN = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i, sample_batched in enumerate(yelpTrainLoader,1):\n",
    "        inputs = sample_batched['image']\n",
    "        labels = sample_batched['labels']\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = alex_net(inputs)\n",
    "        loss = criterion(outputs.float(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss+= loss.item()\n",
    "        pred = (torch.sigmoid(outputs).data > 0.5).int()\n",
    "        #print(pred)\n",
    "        labels = labels.int()\n",
    "        #print(labels)\n",
    "        \n",
    "        TP += ((pred == 1) & (labels == 1) ).float().sum() #True Positive Count\n",
    "        TN += ((pred == 0) & (labels == 0) ).float().sum() #True Negative Count\n",
    "        FP += ((pred == 1) & (labels == 0) ).float().sum() #False Positive Count\n",
    "        FN += ((pred == 0) & (labels == 1) ).float().sum() #False Negative Count\n",
    "        #print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP,TN,FP,FN) )    \n",
    "    \n",
    "    accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "    precision = TP/(TP + FP)\n",
    "    recall = TP/(TP + FN)\n",
    "    f1_score = 2 * (precision*recall)/(precision+recall)\n",
    "    acc_data_train.append([TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "    train_loss=float(train_loss)/float(i)\n",
    "    train_losses.append(train_loss)\n",
    "    # print statistics        \n",
    "    print('Train Epoch:{}  Accuracy: {:.2f}   Average Loss: {:.2f}  Precision: {:.2f}   F1 Score: {:.2f}\\n'.\n",
    "          format(epoch, accuracy, train_loss, precision, f1_score))\n",
    "    \n",
    "def alex_test(epoch):\n",
    "    #Have our model in evaluation mode\n",
    "    alex_net.eval()\n",
    "    #Set losses and Correct labels to zero\n",
    "    test_loss = 0\n",
    "    TN = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(yelpTestLoader,1):\n",
    "            inputs = sample_batched['image']\n",
    "            labels = sample_batched['labels']\n",
    "            outputs = alex_net(inputs)\n",
    "            loss = criterion(outputs.float(), labels.float())\n",
    "            test_loss += loss.item()\n",
    "            pred = (torch.sigmoid(outputs).data > 0.5).int()\n",
    "            #print(pred)\n",
    "            labels = labels.int()\n",
    "            #print(labels)\n",
    "\n",
    "            TP += ((pred == 1) & (labels == 1) ).float().sum() #True Positive Count\n",
    "            TN += ((pred == 0) & (labels == 0) ).float().sum() #True Negative Count\n",
    "            FP += ((pred == 1) & (labels == 0) ).float().sum() #False Positive Count\n",
    "            FN += ((pred == 0) & (labels == 1) ).float().sum() #False Negative Count\n",
    "            #print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP,TN,FP,FN) )    \n",
    "\n",
    "        accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "        precision = TP/(TP + FP)\n",
    "        recall = TP/(TP + FN)\n",
    "        f1_score = 2 * (precision*recall)/(precision+recall)\n",
    "        acc_data_test.append([TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "        test_loss = float(test_loss)/float(i)\n",
    "        test_losses.append(test_loss)\n",
    "        # print statistics        \n",
    "        print('Test Epoch:{}  Accuracy: {:.2f}   Average Loss: {:.2f}  Precision: {:.2f}   F1 Score: {:.2f}\\n'.\n",
    "              format(epoch, accuracy, test_loss, precision, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: SGD\tLearning Rate: 0.1\tMomentum: 0.3\n",
      "Train Epoch:4  Accuracy: 0.54   Average Loss: 1.27  Precision: 0.55   F1 Score: 0.56\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.1\tMomentum: 0.4\n",
      "Train Epoch:4  Accuracy: 0.60   Average Loss: 1.09  Precision: 0.61   F1 Score: 0.62\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.1\tMomentum: 0.5\n",
      "Train Epoch:4  Accuracy: 0.58   Average Loss: 1.27  Precision: 0.59   F1 Score: 0.59\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.1\tMomentum: 0.6\n",
      "Train Epoch:4  Accuracy: 0.58   Average Loss: 1.44  Precision: 0.59   F1 Score: 0.60\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.1\tMomentum: 0.7\n",
      "Train Epoch:4  Accuracy: 0.58   Average Loss: 1.41  Precision: 0.59   F1 Score: 0.60\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.1\tMomentum: 0.8\n",
      "Train Epoch:4  Accuracy: 0.56   Average Loss: 1.82  Precision: 0.57   F1 Score: 0.57\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.1\tMomentum: 0.9\n",
      "Train Epoch:4  Accuracy: 0.57   Average Loss: 2.10  Precision: 0.58   F1 Score: 0.58\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.01\tMomentum: 0.3\n",
      "Train Epoch:4  Accuracy: 0.60   Average Loss: 0.69  Precision: 0.61   F1 Score: 0.61\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.01\tMomentum: 0.4\n",
      "Train Epoch:4  Accuracy: 0.58   Average Loss: 0.71  Precision: 0.60   F1 Score: 0.60\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.01\tMomentum: 0.5\n",
      "Train Epoch:4  Accuracy: 0.61   Average Loss: 0.70  Precision: 0.61   F1 Score: 0.63\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.01\tMomentum: 0.6\n",
      "Train Epoch:4  Accuracy: 0.57   Average Loss: 0.72  Precision: 0.58   F1 Score: 0.59\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.01\tMomentum: 0.7\n",
      "Train Epoch:4  Accuracy: 0.59   Average Loss: 0.70  Precision: 0.60   F1 Score: 0.60\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.01\tMomentum: 0.8\n",
      "Train Epoch:4  Accuracy: 0.60   Average Loss: 0.70  Precision: 0.62   F1 Score: 0.61\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.01\tMomentum: 0.9\n",
      "Train Epoch:4  Accuracy: 0.60   Average Loss: 0.71  Precision: 0.62   F1 Score: 0.61\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.001\tMomentum: 0.3\n",
      "Train Epoch:4  Accuracy: 0.53   Average Loss: 0.72  Precision: 0.53   F1 Score: 0.57\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.001\tMomentum: 0.4\n",
      "Train Epoch:4  Accuracy: 0.51   Average Loss: 0.74  Precision: 0.53   F1 Score: 0.50\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.001\tMomentum: 0.5\n",
      "Train Epoch:4  Accuracy: 0.52   Average Loss: 0.74  Precision: 0.53   F1 Score: 0.55\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.001\tMomentum: 0.6\n",
      "Train Epoch:4  Accuracy: 0.55   Average Loss: 0.72  Precision: 0.57   F1 Score: 0.53\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.001\tMomentum: 0.7\n",
      "Train Epoch:4  Accuracy: 0.53   Average Loss: 0.72  Precision: 0.54   F1 Score: 0.56\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.001\tMomentum: 0.8\n",
      "Train Epoch:4  Accuracy: 0.50   Average Loss: 0.74  Precision: 0.52   F1 Score: 0.52\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 0.001\tMomentum: 0.9\n",
      "Train Epoch:4  Accuracy: 0.52   Average Loss: 0.74  Precision: 0.53   F1 Score: 0.54\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 1e-05\tMomentum: 0.3\n",
      "Train Epoch:4  Accuracy: 0.48   Average Loss: 0.78  Precision: 0.50   F1 Score: 0.43\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 1e-05\tMomentum: 0.4\n",
      "Train Epoch:4  Accuracy: 0.50   Average Loss: 0.76  Precision: 0.51   F1 Score: 0.54\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 1e-05\tMomentum: 0.5\n",
      "Train Epoch:4  Accuracy: 0.53   Average Loss: 0.73  Precision: 0.55   F1 Score: 0.52\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 1e-05\tMomentum: 0.6\n",
      "Train Epoch:4  Accuracy: 0.49   Average Loss: 0.77  Precision: 0.51   F1 Score: 0.51\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 1e-05\tMomentum: 0.7\n",
      "Train Epoch:4  Accuracy: 0.50   Average Loss: 0.77  Precision: 0.51   F1 Score: 0.52\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 1e-05\tMomentum: 0.8\n",
      "Train Epoch:4  Accuracy: 0.51   Average Loss: 0.73  Precision: 0.52   F1 Score: 0.55\n",
      "\n",
      "Optimizer: SGD\tLearning Rate: 1e-05\tMomentum: 0.9\n",
      "Train Epoch:4  Accuracy: 0.52   Average Loss: 0.74  Precision: 0.53   F1 Score: 0.55\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.1\t WeightDecay: 0.1\n",
      "Train Epoch:4  Accuracy: 0.56   Average Loss: 11.87  Precision: 0.57   F1 Score: 0.57\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.1\t WeightDecay: 0.01\n",
      "Train Epoch:4  Accuracy: 0.57   Average Loss: 15.07  Precision: 0.59   F1 Score: 0.58\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.1\t WeightDecay: 0.001\n",
      "Train Epoch:4  Accuracy: 0.59   Average Loss: 18.09  Precision: 0.60   F1 Score: 0.59\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.01\t WeightDecay: 0.1\n",
      "Train Epoch:4  Accuracy: 0.59   Average Loss: 1.71  Precision: 0.61   F1 Score: 0.60\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.01\t WeightDecay: 0.01\n",
      "Train Epoch:4  Accuracy: 0.60   Average Loss: 1.72  Precision: 0.62   F1 Score: 0.60\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.01\t WeightDecay: 0.001\n",
      "Train Epoch:4  Accuracy: 0.59   Average Loss: 1.97  Precision: 0.60   F1 Score: 0.60\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.001\t WeightDecay: 0.1\n",
      "Train Epoch:4  Accuracy: 0.61   Average Loss: 0.73  Precision: 0.63   F1 Score: 0.62\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.001\t WeightDecay: 0.01\n",
      "Train Epoch:4  Accuracy: 0.58   Average Loss: 0.75  Precision: 0.58   F1 Score: 0.61\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 0.001\t WeightDecay: 0.001\n",
      "Train Epoch:4  Accuracy: 0.60   Average Loss: 0.77  Precision: 0.61   F1 Score: 0.62\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 1e-05\t WeightDecay: 0.1\n",
      "Train Epoch:4  Accuracy: 0.50   Average Loss: 0.75  Precision: 0.52   F1 Score: 0.50\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 1e-05\t WeightDecay: 0.01\n",
      "Train Epoch:4  Accuracy: 0.49   Average Loss: 0.77  Precision: 0.50   F1 Score: 0.48\n",
      "\n",
      "Optimizer: Adam\t Learning Rate: 1e-05\t WeightDecay: 0.001\n",
      "Train Epoch:4  Accuracy: 0.49   Average Loss: 0.75  Precision: 0.51   F1 Score: 0.49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter tuning - Grid Search \n",
    "\n",
    "learning_rates = [.1,.01,.001, .00001]\n",
    "momentums = [.3,.4,.5,.6, .7, .8, .9]\n",
    "w = [.1,.01,.001]\n",
    "\n",
    "for i in [1,2]: #Test different optimizer\n",
    "    for j in range(len(learning_rates)):    \n",
    "        if i ==1:\n",
    "            for k in range(len(momentums)):\n",
    "                #Reset Model per test\n",
    "                alex_net = models.alexnet(pretrained=True)\n",
    "                for param in alex_net.parameters():\n",
    "                    param.requires_grad = False\n",
    "                alex_net.classifier._modules['6'] = nn.Linear(4096, 9)\n",
    "                \n",
    "                optimizer = optim.SGD(alex_net.classifier._modules['6'].parameters(), \n",
    "                                  lr=learning_rates[j], momentum=momentums[k])\n",
    "                \n",
    "                print('Optimizer: SGD\\tLearning Rate: {}\\tMomentum: {}'.\n",
    "                      format (learning_rates[j], momementums[k]))\n",
    "                alex_train(epoch)\n",
    "                \n",
    "        else: \n",
    "            for k in range(len(w)):\n",
    "                #Reset Model per test\n",
    "                alex_net = models.alexnet(pretrained=True)\n",
    "                for param in alex_net.parameters():\n",
    "                    param.requires_grad = False\n",
    "                alex_net.classifier._modules['6'] = nn.Linear(4096, 9)\n",
    "                \n",
    "                optimizer = optim.Adam(alex_net.classifier._modules['6'].parameters(), \n",
    "                                  lr=learning_rates[j], weight_decay = w[k], amsgrad=True)\n",
    "        \n",
    "                print('Optimizer: Adam\\t Learning Rate: {}\\t WeightDecay: {}'.\n",
    "                      format (learning_rates[j], w[k]))\n",
    "                alex_train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch:0  Accuracy: 0.51   Average Loss: 0.74  Precision: 0.54   F1 Score: 0.53\n",
      "\n",
      "Train Epoch:1  Accuracy: 0.69   Average Loss: 0.61  Precision: 0.70   F1 Score: 0.70\n",
      "\n",
      "Test Epoch:1  Accuracy: 0.72   Average Loss: 0.54  Precision: 0.72   F1 Score: 0.74\n",
      "\n",
      "Train Epoch:2  Accuracy: 0.69   Average Loss: 0.60  Precision: 0.70   F1 Score: 0.71\n",
      "\n",
      "Test Epoch:2  Accuracy: 0.72   Average Loss: 0.55  Precision: 0.72   F1 Score: 0.74\n",
      "\n",
      "Train Epoch:3  Accuracy: 0.69   Average Loss: 0.60  Precision: 0.71   F1 Score: 0.71\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alex_net = models.alexnet(pretrained=True)\n",
    "for param in alex_net.parameters():\n",
    "    param.requires_grad = False\n",
    "alex_net.classifier._modules['6'] = nn.Linear(4096, 9)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(alex_net.classifier._modules['6'].parameters(), \n",
    "                                  lr=.001, weight_decay = .1, amsgrad=True)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "acc_data_train=[]\n",
    "acc_data_test=[]\n",
    "\n",
    "alex_test(0)\n",
    "for epoch in range(1,6):\n",
    "    alex_train(epoch)\n",
    "    alex_test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
